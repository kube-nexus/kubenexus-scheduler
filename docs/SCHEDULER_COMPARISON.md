# Kubernetes Scheduler Comparison

A technical comparison of advanced Kubernetes schedulers for batch workloads, GPU scheduling, and multi-tenancy.

---

## Quick Comparison

| Feature | **KubeNexus** | **Kueue** | **YuniKorn** |
|---------|---------------|-----------|--------------|
| **Primary Focus** | Hybrid scheduling (batch + service) | Job queueing & quota management | Multi-tenant resource scheduling |
| **Gang Scheduling** | ‚úÖ Built-in | ‚úÖ Via integration | ‚úÖ Native support |
| **GPU Topology** | ‚úÖ NUMA + PCIe aware | ‚ùå Limited | ‚ö†Ô∏è CPU only |
| **Preemption** | ‚úÖ Policy-based | ‚úÖ Queue priority | ‚úÖ Fair-share preemption |
| **Workload Classification** | ‚úÖ Automatic | ‚ùå Manual queue assignment | ‚ö†Ô∏è Application-based |
| **Multi-tenancy** | ‚ö†Ô∏è Basic namespaces | ‚úÖ Strong queue hierarchies | ‚úÖ Strong hierarchies |
| **Deployment** | Plugin (easy) | Controller + CRDs | Replace scheduler |
| **Maturity** | üÜï Beta (v0.1.x) | ‚úÖ Production | ‚úÖ Production |

---

## 1. Kueue (Kubernetes SIG)

### Overview
**Kueue** is a Kubernetes-native job queueing system for **quota management and fair sharing**. It works on top of the default Kubernetes scheduler.

### Architecture
```
Job ‚Üí Kueue Admission ‚Üí Queue ‚Üí Quota Check ‚Üí Unsuspend ‚Üí K8s Scheduler
```

### Key Features
- **Hierarchical queues** (ClusterQueue ‚Üí LocalQueue)
- **Resource quotas** per tenant/team
- **Job prioritization** within queues
- **AdmissionChecks** for pre-scheduling validation
- **Integration** with Kubeflow, Spark, Ray operators

### Best For
‚úÖ Multi-tenant GPU clusters with strict quotas  
‚úÖ Fair sharing across teams/projects  
‚úÖ Cost optimization (preemptible vs on-demand)  
‚úÖ GKE/Autopilot integration

### Limitations
‚ùå Not a scheduler (relies on default K8s scheduler)  
‚ùå No topology awareness (NUMA, PCIe locality)  
‚ùå Manual queue assignment required

---

## 2. YuniKorn (Apache)

### Overview
**YuniKorn** is a standalone scheduler (replaces kube-scheduler) designed for **big data and ML workloads** at scale.

### Architecture
```
Job ‚Üí YuniKorn Scheduler ‚Üí Gang Scheduling ‚Üí Placement ‚Üí Binding
```

### Key Features
- **Native gang scheduling** with placeholder pods
- **Hierarchical queues** with guaranteed/max resources
- **Fair-share scheduling** (DRF algorithm)
- **GPU scheduling** with topology and sharing support
- **Application-level tracking** (not just pods)

### Best For
‚úÖ Spark on Kubernetes (best-in-class)  
‚úÖ Large-scale clusters (100+ GPUs, 5000+ nodes)  
‚úÖ Multi-tenant big data platforms  
‚úÖ Fair-share with strong SLAs

### Limitations
‚ùå Replaces kube-scheduler (operational complexity)  
‚ùå Steep learning curve (Hadoop-style configs)  
‚ùå Overkill for simple use cases

---

## 3. KubeNexus

### Overview
**KubeNexus** is a lightweight scheduler plugin providing **automatic workload classification and topology-aware scheduling** for hybrid workloads.

### Architecture
```
Pod ‚Üí Classification ‚Üí Topology Scoring ‚Üí Gang Scheduling ‚Üí Binding
```

### Key Features
- **Automatic workload detection** (Spark, TensorFlow, PyTorch, Ray)
- **GPU topology awareness** (NUMA, PCIe, NVLink)
- **Hybrid scoring** (spread services, pack batch jobs)
- **Resource reservation** for driver pods
- **Gang scheduling** via coscheduling plugin

### Best For
‚úÖ Mixed service + batch workloads  
‚úÖ GPU topology optimization  
‚úÖ Simple deployment (no CRDs)  
‚úÖ Automatic workload handling

### Limitations
‚ö†Ô∏è Beta status (v0.1.x)  
‚ùå No built-in quota management  
‚ùå Basic multi-tenancy only  
‚ùå Not production-proven yet

---

## Feature Comparison

### Gang Scheduling

| Scheduler | Implementation | Timeout | Placeholders | Maturity |
|-----------|----------------|---------|--------------|----------|
| **Kueue** | Via JobSet/operators | ‚úÖ Configurable | ‚ùå No | ‚úÖ High |
| **YuniKorn** | Native, app-level | ‚úÖ Advanced | ‚úÖ Yes | ‚úÖ High |
| **KubeNexus** | Coscheduling plugin | ‚ö†Ô∏è Basic | ‚ùå No | ‚ö†Ô∏è Beta |

**Winner**: YuniKorn (most mature)

### GPU Topology Awareness

| Feature | Kueue | YuniKorn | KubeNexus |
|---------|-------|----------|-----------|
| **NUMA locality** | ‚ùå | ‚ö†Ô∏è CPU only | ‚úÖ GPU + CPU |
| **PCIe topology** | ‚ùå | ‚ö†Ô∏è Basic | ‚úÖ Advanced |
| **NVLink detection** | ‚ùå | ‚ùå | ‚úÖ Yes |
| **GPU sharing** | ‚ö†Ô∏è Time-slicing | ‚úÖ MIG + time-slicing | ‚ö†Ô∏è External |

**Winner**: KubeNexus (topology), YuniKorn (sharing)

### Multi-Tenancy & Quotas

| Feature | Kueue | YuniKorn | KubeNexus |
|---------|-------|----------|-----------|
| **Hierarchical queues** | ‚úÖ Yes | ‚úÖ Yes | ‚ùå No |
| **Resource quotas** | ‚úÖ Strong | ‚úÖ Strong | ‚ö†Ô∏è K8s only |
| **Fair sharing** | ‚úÖ Weighted | ‚úÖ DRF | ‚ùå No |
| **Preemption** | ‚úÖ Queue priority | ‚úÖ Fair-share | ‚úÖ Basic |

**Winner**: Kueue & YuniKorn (tie)

---

## Use Case Recommendations

### Large-Scale ML Platform (500+ GPUs, 10+ teams)
**Recommended**: **Kueue + YuniKorn**
- Kueue for quota management
- YuniKorn for gang scheduling and fair-share
- Proven at scale

### Spark on Kubernetes
**Recommended**: **YuniKorn**
- Best Spark integration
- Native gang scheduling for executors
- Application-level tracking

### Mixed Service + Batch Workloads
**Recommended**: **KubeNexus**
- Automatic workload classification
- Hybrid scoring (spread vs pack)
- Simple deployment

### Multi-GPU Topology Optimization
**Recommended**: **KubeNexus**
- NUMA + PCIe aware
- NVLink detection
- GPU-CPU affinity

### Enterprise Multi-Tenancy
**Recommended**: **Kueue or YuniKorn**
- Strong quota management
- Hierarchical queues
- Fair-share scheduling

---

## Combining Schedulers

### Architecture: Kueue + KubeNexus
```
Job ‚Üí Kueue (admission + quota) ‚Üí KubeNexus (topology placement) ‚Üí Pods
```

**Benefits**:
- Kueue handles multi-tenancy and quotas
- KubeNexus optimizes GPU topology
- Best of both worlds

**Configuration**:
```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: gpu-queue
spec:
  clusterQueue: gpu-cluster-queue
---
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    kueue.x-k8s.io/queue-name: gpu-queue
spec:
  template:
    spec:
      schedulerName: kubenexus-scheduler  # Use KubeNexus for placement
      containers:
      - name: training
        resources:
          requests:
            nvidia.com/gpu: 4
```

---

## Technical Details

### Kueue Gang Scheduling
Uses JobSet or operator integration:
```yaml
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  name: distributed-training
spec:
  successPolicy:
    operator: All  # All jobs must succeed
  replicatedJobs:
  - name: workers
    replicas: 8
```

### YuniKorn Gang Scheduling
Native with placeholders:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    applicationId: spark-app-001
    task-group.kubenexus.io/name: executors
    task-group.kubenexus.io/minMember: "8"
```

### KubeNexus Gang Scheduling
Pod-group annotations:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-group.scheduling.kubenexus.io/name: training-001
    pod-group.scheduling.kubenexus.io/min-available: "8"
spec:
  schedulerName: kubenexus-scheduler
```

---

## Performance Considerations

### Scheduling Latency

| Scheduler | Single Pod | Gang (8 pods) | Notes |
|-----------|------------|---------------|-------|
| **Kueue** | ~5-10ms overhead | Depends on operator | Admission overhead |
| **YuniKorn** | ~5-15ms | ~50-100ms | Placeholder creation |
| **KubeNexus** | ~5-10ms | ~50ms (target) | Plugin overhead |

### Resource Overhead

| Scheduler | Memory | CPU | Storage |
|-----------|--------|-----|---------|
| **Kueue** | ~100MB | <0.1 core | CRD state |
| **YuniKorn** | ~200MB | <0.5 core | In-memory |
| **KubeNexus** | ~50MB | <0.1 core | In-memory |

---

## Migration Path

### From Default Scheduler to KubeNexus
1. Deploy KubeNexus alongside default scheduler
2. Test with specific workloads (`schedulerName: kubenexus-scheduler`)
3. Gradually migrate workloads
4. No CRD migration needed

### From Kueue to Kueue + KubeNexus
1. Keep Kueue for admission control
2. Add KubeNexus for placement
3. Update pod templates to use `kubenexus-scheduler`
4. Kueue continues managing quotas

### From Default to YuniKorn
1. Deploy YuniKorn
2. Configure queues
3. Update workloads to use `schedulerName: yunikorn`
4. Requires complete scheduler replacement

---

## Recommendations by Cluster Size

### Small (< 50 nodes, < 20 GPUs)
**Recommended**: **KubeNexus** or **Default + ResourceQuotas**
- Low complexity
- Simple topology optimization
- No need for complex multi-tenancy

### Medium (50-500 nodes, 20-100 GPUs)
**Recommended**: **Kueue + KubeNexus**
- Kueue for quota management
- KubeNexus for topology
- Balanced complexity vs features

### Large (500+ nodes, 100+ GPUs)
**Recommended**: **Kueue + YuniKorn** or **YuniKorn only**
- Proven at scale
- Strong multi-tenancy
- Mature gang scheduling

---

## References

- [Kueue Documentation](https://kueue.sigs.k8s.io/)
- [YuniKorn Documentation](https://yunikorn.apache.org/)
- [Kubernetes Scheduler Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- [Gang Scheduling in Kubernetes](https://github.com/kubernetes-sigs/scheduler-plugins/tree/master/pkg/coscheduling)

---

## Summary

| Scenario | Best Choice | Reason |
|----------|-------------|--------|
| **Production-scale multi-tenancy** | Kueue or YuniKorn | Proven quota management |
| **Spark workloads** | YuniKorn | Best Spark integration |
| **GPU topology optimization** | KubeNexus | Advanced NUMA/PCIe awareness |
| **Mixed service + batch** | KubeNexus | Automatic workload classification |
| **Large-scale (1000+ nodes)** | YuniKorn | Proven scalability |
| **Simple deployment** | KubeNexus | Lightweight plugin |

**KubeNexus Status**: Beta (v0.1.x) - Suitable for testing and early adoption. For production-critical workloads, consider mature alternatives like Kueue or YuniKorn until KubeNexus reaches v1.0.
