# KubeNexus Scheduler

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?logo=go)](https://golang.org/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.28+-326CE5?logo=kubernetes)](https://kubernetes.io/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
[![Status](https://img.shields.io/badge/Status-Beta-yellow.svg)]()

**A Kubernetes scheduler for modern workloads‚Äîfrom stateless microservices to batch jobs to GPU-intensive AI training.**

> **‚ö†Ô∏è Beta Status**: KubeNexus Scheduler is under active development (v0.1.x). It's ready for testing in dev/staging environments and suitable for early adopters. Production use should be carefully evaluated for your specific use case.

KubeNexus extends the Kubernetes scheduler with intelligent workload placement, gang scheduling, and NUMA topology awareness. Built on the native Scheduler Framework, it delivers advanced scheduling capabilities with minimal operational overhead.

---

## Why KubeNexus?

Modern Kubernetes clusters run diverse workloads with different scheduling requirements:

- **Stateless Services**: Need fast scheduling and efficient bin-packing
- **Batch Jobs**: Require gang scheduling to prevent resource deadlocks
- **GPU Workloads**: Demand topology-aware placement for optimal performance
- **Mixed Environments**: Need a single scheduler that handles all scenarios

KubeNexus provides a unified scheduling solution that adapts to your workload characteristics without complex configuration or multiple schedulers.

---

## Key Features

### üéØ Gang Scheduling (Co-scheduling)

Schedule pod groups atomically‚Äîall pods in a group start together or none at all. Essential for distributed workloads where partial scheduling causes deadlocks and resource waste.

**Perfect for:**
- Distributed ML training (PyTorch DDP, TensorFlow, Horovod)
- **Kubeflow Training Operator** (PyTorchJob, TFJob, MPIJob)
- **Spark Operator** (SparkApplication)
- MPI applications
- Ray clusters
- Any multi-pod application requiring coordination

```yaml
labels:
  pod-group.scheduling.kubenexus.io/name: "training-job"
  pod-group.scheduling.kubenexus.io/min-available: "8"
```

**How it works:** Operators create pods from their CRDs (SparkApplication, PyTorchJob, etc.) with your specified labels. KubeNexus schedules these pods‚Äîno operator changes needed. Works with any operator out of the box.

See: [Kubeflow Integration](docs/KUBEFLOW_INTEGRATION.md) | [Spark Integration](docs/SPARK_OPERATOR_INTEGRATION.md) | [How Operators Work](docs/OPERATOR_CRD_SUPPORT.md)

### üß† NUMA-Aware Scheduling

Optimize pod placement based on CPU, memory, and GPU topology for maximum performance. Reduces cross-NUMA memory access, minimizes PCIe latency, and maximizes GPU training throughput.

**Perfect for:**
- GPU-accelerated AI/ML training
- High-performance computing (HPC)
- Real-time inference workloads
- Low-latency applications

```yaml
annotations:
  numa.scheduling.kubenexus.io/policy: "single-numa"
  numa.scheduling.kubenexus.io/resources: "cpu,memory,nvidia.com/gpu"
```

**Benefits:**
- 2-3x faster GPU training through optimal placement
- Lower memory latency for HPC workloads
- Better GPU utilization through PCIe topology awareness

### ‚öñÔ∏è Intelligent Queue Management

- **Starvation prevention**: Automatic priority boost for waiting pods
- **FIFO fairness**: Age-based scheduling within priority classes
- **Priority-aware**: Respects Kubernetes pod priorities
- **Deadlock resolution**: Smart preemption for gang scheduling

### üöÄ Deployment-Ready Features

- **High availability**: Built-in leader election for multi-replica deployments
- **Zero dependencies**: No external databases or coordination services
- **Minimal footprint**: ~50MB memory, negligible CPU overhead
- **Native integration**: Built on Kubernetes Scheduler Framework v1.28+

---

## Quick Start

### Installation

```bash
# Deploy KubeNexus scheduler
kubectl apply -f https://raw.githubusercontent.com/gouthamreddykotapalle/kubenexus-scheduler/main/deploy/kubenexus-scheduler.yaml

# Verify deployment
kubectl get pods -n kube-system -l app=kubenexus-scheduler
```

### Example 1: Distributed Training (Gang Scheduling)

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: distributed-training
spec:
  parallelism: 8
  completions: 8
  template:
    metadata:
      labels:
        pod-group.scheduling.kubenexus.io/name: "training-job"
        pod-group.scheduling.kubenexus.io/min-available: "8"
    spec:
      schedulerName: kubenexus-scheduler
      containers:
      - name: worker
        image: pytorch/pytorch:latest
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
            nvidia.com/gpu: "1"
```

**Result**: All 8 workers start simultaneously, preventing deadlock and resource waste.

### Example 2: GPU Training (NUMA-Aware)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
  annotations:
    numa.scheduling.kubenexus.io/policy: "single-numa"
    numa.scheduling.kubenexus.io/resources: "cpu,memory,nvidia.com/gpu"
spec:
  schedulerName: kubenexus-scheduler
  containers:
  - name: training
    image: nvcr.io/nvidia/pytorch:latest
    resources:
      requests:
        cpu: "16"
        memory: "64Gi"
        nvidia.com/gpu: "2"
```

**Result**: CPUs, memory, and GPUs allocated from same NUMA node for optimal performance.

### Example 3: Stateless Service (Default Scheduling)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      schedulerName: kubenexus-scheduler
      containers:
      - name: api
        image: my-api:latest
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
```

**Result**: Fast, efficient bin-packing with standard Kubernetes scheduling behavior.

---

## Use Cases

### Distributed Machine Learning

**Challenge**: Training large models requires coordinating multiple GPU workers. Partial scheduling leads to deadlock, wasting expensive GPU resources.

**Solution**: Gang scheduling ensures all workers start together. NUMA awareness optimizes GPU placement.

**Example Workloads**: PyTorch Distributed, TensorFlow Training, Horovod, DeepSpeed

### Apache Spark on Kubernetes

**Challenge**: Spark jobs need driver + executors scheduled together. Default scheduler can deadlock when cluster is near capacity.

**Solution**: Gang scheduling with driver and all executors as a pod group.

**Example Workloads**: Spark batch jobs, Spark Structured Streaming, PySpark

### High-Performance Computing (HPC)

**Challenge**: HPC applications are sensitive to memory latency and CPU topology.

**Solution**: NUMA-aware scheduling with isolated or single-NUMA policies for predictable performance.

**Example Workloads**: Molecular dynamics, CFD simulations, finite element analysis

### AI Inference

**Challenge**: Real-time inference needs low latency and consistent performance.

**Solution**: NUMA-aware placement with GPU and CPU on same NUMA node reduces PCIe latency.

**Example Workloads**: Real-time video processing, LLM inference, recommendation systems

### Mixed Workloads

**Challenge**: Clusters run diverse workloads‚Äîmicroservices, batch jobs, and GPU training‚Äîeach with different needs.

**Solution**: Single scheduler that adapts to workload characteristics without reconfiguration.

**Example**: Production services + nightly batch jobs + ML training on the same cluster

---

## Architecture

KubeNexus is implemented as a set of plugins for the Kubernetes Scheduler Framework:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              KubeNexus Scheduler                        ‚îÇ
‚îÇ              (Kubernetes Scheduler Framework)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                 ‚îÇ                 ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Gang    ‚îÇ      ‚îÇ   NUMA    ‚îÇ    ‚îÇ  Queue     ‚îÇ
   ‚îÇScheduler‚îÇ      ‚îÇ Topology  ‚îÇ    ‚îÇ Management ‚îÇ
   ‚îÇ         ‚îÇ      ‚îÇ           ‚îÇ    ‚îÇ            ‚îÇ
   ‚îÇ‚Ä¢ Filter ‚îÇ      ‚îÇ ‚Ä¢ Filter  ‚îÇ    ‚îÇ ‚Ä¢ Priority ‚îÇ
   ‚îÇ‚Ä¢ Permit ‚îÇ      ‚îÇ ‚Ä¢ Score   ‚îÇ    ‚îÇ ‚Ä¢ Fairness ‚îÇ
   ‚îÇ‚Ä¢ Reserve‚îÇ      ‚îÇ ‚Ä¢ Binding ‚îÇ    ‚îÇ ‚Ä¢ Backfill ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Coscheduling Plugin**: Implements gang scheduling with permit phase coordination
**NUMA Topology Plugin**: Scores nodes based on topology affinity
**Queue Management**: Prevents starvation and ensures fairness

For NUMA architecture details, see [NUMA Scheduling Guide](docs/NUMA_SCHEDULING_GUIDE.md).

---

## Performance

> **Note**: Performance metrics below are design targets based on the Kubernetes Scheduler Framework. Actual performance in your environment may vary. We welcome benchmarking contributions!

### Scheduling Latency (Target)

| Workload Type | Pods | Latency (p50) | Latency (p99) |
|---------------|------|---------------|---------------|
| Stateless     | 1    | ~5ms          | ~15ms         |
| Gang (8 pods) | 8    | ~50ms         | ~150ms        |
| NUMA-aware    | 1    | ~8ms          | ~25ms         |

### Resource Overhead (Estimated)

| Metric | Value |
|--------|-------|
| Memory | ~50MB |
| CPU    | <0.1 core (idle), <0.5 core (high load) |
| Storage | None (in-memory state only) |

### Scalability (Design Goals)

Designed to handle:
- 1,000+ nodes
- 10,000+ pods scheduled
- 100+ concurrent gang scheduling groups
- Sub-second gang formation time for groups up to 50 pods

*Benchmark results and real-world performance data welcome‚Äîplease share your findings!*

---

## Comparison with Alternatives

| Feature | KubeNexus | Volcano | YuniKorn | Kueue |
|---------|-----------|---------|----------|-------|
| **Gang Scheduling** | ‚úÖ Built-in | ‚úÖ Yes | ‚úÖ Yes | ‚ö†Ô∏è Via adapter |
| **NUMA Topology** | ‚úÖ Full support | ‚ùå No | ‚ö†Ô∏è CPU only | ‚ùå No |
| **GPU Awareness** | ‚úÖ PCIe topology | ‚ö†Ô∏è Basic | ‚úÖ Yes | ‚ùå No |
| **Stateless Workloads** | ‚úÖ Native | ‚ö†Ô∏è Overhead | ‚ö†Ô∏è Overhead | ‚ùå Not designed for |
| **Setup Complexity** | üü¢ Low | üü° Medium | üî¥ High | üü° Medium |
| **External Dependencies** | üü¢ None | üü° CRDs | üî¥ Many | üü° CRDs |
| **Best For** | Mixed workloads | Batch jobs | Large multi-tenant | Quota management |

### When to Choose KubeNexus

‚úÖ **Choose KubeNexus if you need:**
- Gang scheduling for distributed ML/Spark/MPI workloads
- NUMA-aware scheduling for GPU/HPC performance
- A single scheduler for stateless + batch + GPU workloads
- Minimal operational complexity
- Quick setup and low resource overhead

‚ö†Ô∏è **Consider alternatives if you need:**
- Advanced multi-tenant fair-share policies (‚Üí YuniKorn)
- Complex quota hierarchies (‚Üí Kueue)
- Workflow orchestration (‚Üí Volcano + Argo)
- Support for >5000 node clusters (‚Üí YuniKorn)

---

## Documentation

| Document | Description |
|----------|-------------|
| [**User Guide**](docs/USER_GUIDE.md) | Complete guide with examples and troubleshooting |
| [**Kubeflow Integration**](docs/KUBEFLOW_INTEGRATION.md) | Using KubeNexus with Kubeflow Training Operator |
| [**Spark Operator Integration**](docs/SPARK_OPERATOR_INTEGRATION.md) | Complete Spark on Kubernetes guide |
| [**Operator CRD Support**](docs/OPERATOR_CRD_SUPPORT.md) | How KubeNexus works with any Kubernetes operator |
| [**NUMA Scheduling Guide**](docs/NUMA_SCHEDULING_GUIDE.md) | Deep dive into NUMA-aware scheduling |
| [**NUMA Quick Reference**](docs/NUMA_QUICK_REFERENCE.md) | Cheat sheet for common tasks |
| [**Scheduler Comparison**](docs/SCHEDULER_COMPARISON.md) | Detailed comparison vs alternatives |
| [**Design Decisions**](docs/DESIGN_DECISIONS.md) | Architecture and API design rationale |

---

## Configuration

### Basic Configuration

KubeNexus works out-of-the-box with sensible defaults. For advanced configuration, edit `config/config.yaml`:

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: kubenexus-scheduler
    plugins:
      queueSort:
        enabled:
          - name: Coscheduling
      preFilter:
        enabled:
          - name: Coscheduling
      permit:
        enabled:
          - name: Coscheduling
      score:
        enabled:
          - name: NUMATopology
            weight: 10  # Increase for stronger NUMA preference
```

### Pod Annotations

```yaml
# Gang scheduling (use labels)
labels:
  pod-group.scheduling.kubenexus.io/name: "<group-name>"
  pod-group.scheduling.kubenexus.io/min-available: "<count>"

# NUMA scheduling (use annotations)
annotations:
  numa.scheduling.kubenexus.io/policy: "best-effort|restricted|single-numa|isolated"
  numa.scheduling.kubenexus.io/resources: "cpu,memory,nvidia.com/gpu"
```

---

## Contributing

We welcome contributions! KubeNexus is designed to be simple, focused, and maintainable.

### Development Setup

```bash
# Clone the repository
git clone https://github.com/YOUR_ORG/kubenexus-scheduler.git
cd kubenexus-scheduler

# Install dependencies
go mod download

# Run tests
make test

# Build
make build

# Run locally (requires kubeconfig)
./bin/kubenexus-scheduler --config=config/config.yaml
```

### Areas for Contribution

- **Testing**: Add integration tests for new scenarios
- **Documentation**: Improve guides and examples
- **Features**: Implement requested features (see [Issues](https://github.com/YOUR_ORG/kubenexus-scheduler/issues))
- **Bug fixes**: Fix reported bugs
- **Performance**: Optimize scheduling algorithms

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## Roadmap

### Current (v0.1.x - Beta)
- ‚úÖ Gang scheduling with permit phase coordination
- ‚úÖ NUMA-aware scheduling with 4 policies
- ‚úÖ Starvation prevention and fairness
- ‚úÖ High availability support
- ‚è≥ Comprehensive testing and benchmarking
- ‚è≥ Production battle-testing

### Planned (v0.5 - Mid 2026)
- ‚è≥ Enhanced metrics and monitoring (Prometheus)
- ‚è≥ Admission webhook for validation
- ‚è≥ Helm chart for easier deployment
- ‚è≥ Namespace-based priority configuration
- ‚è≥ Real-world performance benchmarks

### Future (v1.0 - Late 2026)
- üîÆ Multi-queue support for >5000 node clusters
- üîÆ Advanced fair-share policies
- üîÆ Dynamic resource reservation
- üîÆ Integration with cluster autoscaler
- üîÆ v1.0 stability guarantees and production SLA

See [GitHub Issues](https://github.com/gouthamreddykotapalle/kubenexus-scheduler/issues) for details and discussions.

---

## Community

- **Issues**: [GitHub Issues](https://github.com/gouthamreddykotapalle/kubenexus-scheduler/issues)
- **Discussions**: [GitHub Discussions](https://github.com/gouthamreddykotapalle/kubenexus-scheduler/discussions)
- **Contributing**: See [CONTRIBUTING.md](CONTRIBUTING.md) and [SUPPORT.md](SUPPORT.md)

---

## License

KubeNexus Scheduler is licensed under the [Apache License 2.0](LICENSE).

---

## Acknowledgments

KubeNexus builds upon ideas and patterns from:
- [Kubernetes Scheduler Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- [Kubernetes Scheduler Plugins](https://github.com/kubernetes-sigs/scheduler-plugins)
- [Volcano Scheduler](https://volcano.sh/)
- [Apache YuniKorn](https://yunikorn.apache.org/)

---

<div align="center">

**[Documentation](docs/) ‚Ä¢ [Examples](docs/examples/) ‚Ä¢ [Contributing](CONTRIBUTING.md)**

Made with ‚ù§Ô∏è for the Kubernetes community

‚≠ê **Star us on GitHub if KubeNexus helps your workloads!**

</div>

