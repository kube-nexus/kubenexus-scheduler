# KubeNexus Scheduler

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?logo=go)](https://golang.org/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-1.28+-326CE5?logo=kubernetes)](https://kubernetes.io/)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

**A production-ready Kubernetes scheduler for modern workloadsâ€”from stateless microservices to batch jobs to GPU-intensive AI training.**

KubeNexus extends the Kubernetes scheduler with intelligent workload placement, gang scheduling, and NUMA topology awareness. Built on the native Scheduler Framework, it delivers enterprise-grade scheduling capabilities with minimal operational overhead.

---

## Why KubeNexus?

Modern Kubernetes clusters run diverse workloads with different scheduling requirements:

- **Stateless Services**: Need fast scheduling and efficient bin-packing
- **Batch Jobs**: Require gang scheduling to prevent resource deadlocks
- **GPU Workloads**: Demand topology-aware placement for optimal performance
- **Mixed Environments**: Need a single scheduler that handles all scenarios

KubeNexus provides a unified scheduling solution that adapts to your workload characteristics without complex configuration or multiple schedulers.

---

## Key Features

### ğŸ¯ Gang Scheduling (Co-scheduling)

Schedule pod groups atomicallyâ€”all pods in a group start together or none at all. Essential for distributed workloads where partial scheduling causes deadlocks and resource waste.

**Perfect for:**
- Distributed ML training (PyTorch DDP, TensorFlow, Horovod)
- **Kubeflow Training Operator** (PyTorchJob, TFJob, MPIJob)
- Apache Spark jobs (driver + executors)
- MPI applications
- Ray clusters
- Any multi-pod application requiring coordination

```yaml
labels:
  pod-group.scheduling.kubenexus.io/name: "training-job"
  pod-group.scheduling.kubenexus.io/min-available: "8"
```

**Kubeflow Support**: Native integration with Kubeflow Training Operator CRDs. See [Kubeflow Integration Guide](docs/KUBEFLOW_INTEGRATION.md).

### ğŸ§  NUMA-Aware Scheduling

Optimize pod placement based on CPU, memory, and GPU topology for maximum performance. Reduces cross-NUMA memory access, minimizes PCIe latency, and maximizes GPU training throughput.

**Perfect for:**
- GPU-accelerated AI/ML training
- High-performance computing (HPC)
- Real-time inference workloads
- Low-latency applications

```yaml
annotations:
  numa.scheduling.kubenexus.io/policy: "single-numa"
  numa.scheduling.kubenexus.io/resources: "cpu,memory,nvidia.com/gpu"
```

**Benefits:**
- 2-3x faster GPU training through optimal placement
- Lower memory latency for HPC workloads
- Better GPU utilization through PCIe topology awareness

### âš–ï¸ Intelligent Queue Management

- **Starvation prevention**: Automatic priority boost for waiting pods
- **FIFO fairness**: Age-based scheduling within priority classes
- **Priority-aware**: Respects Kubernetes pod priorities
- **Deadlock resolution**: Smart preemption for gang scheduling

### ğŸš€ Production-Ready

- **High availability**: Built-in leader election for multi-replica deployments
- **Zero dependencies**: No external databases or coordination services
- **Minimal footprint**: ~50MB memory, negligible CPU overhead
- **Native integration**: Built on Kubernetes Scheduler Framework v1.28+

---

## Quick Start

### Installation

```bash
# Deploy KubeNexus scheduler
kubectl apply -f https://raw.githubusercontent.com/YOUR_ORG/kubenexus-scheduler/main/deploy/kubenexus-scheduler.yaml

# Verify deployment
kubectl get pods -n kube-system -l app=kubenexus-scheduler
```

### Example 1: Distributed Training (Gang Scheduling)

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: distributed-training
spec:
  parallelism: 8
  completions: 8
  template:
    metadata:
      labels:
        pod-group.scheduling.kubenexus.io/name: "training-job"
        pod-group.scheduling.kubenexus.io/min-available: "8"
    spec:
      schedulerName: kubenexus-scheduler
      containers:
      - name: worker
        image: pytorch/pytorch:latest
        resources:
          requests:
            cpu: "4"
            memory: "16Gi"
            nvidia.com/gpu: "1"
```

**Result**: All 8 workers start simultaneously, preventing deadlock and resource waste.

### Example 2: GPU Training (NUMA-Aware)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ml-training
  annotations:
    numa.scheduling.kubenexus.io/policy: "single-numa"
    numa.scheduling.kubenexus.io/resources: "cpu,memory,nvidia.com/gpu"
spec:
  schedulerName: kubenexus-scheduler
  containers:
  - name: training
    image: nvcr.io/nvidia/pytorch:latest
    resources:
      requests:
        cpu: "16"
        memory: "64Gi"
        nvidia.com/gpu: "2"
```

**Result**: CPUs, memory, and GPUs allocated from same NUMA node for optimal performance.

### Example 3: Stateless Service (Default Scheduling)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-server
spec:
  replicas: 10
  template:
    spec:
      schedulerName: kubenexus-scheduler
      containers:
      - name: api
        image: my-api:latest
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
```

**Result**: Fast, efficient bin-packing with standard Kubernetes scheduling behavior.

---

## Use Cases

### Distributed Machine Learning

**Challenge**: Training large models requires coordinating multiple GPU workers. Partial scheduling leads to deadlock, wasting expensive GPU resources.

**Solution**: Gang scheduling ensures all workers start together. NUMA awareness optimizes GPU placement.

**Example Workloads**: PyTorch Distributed, TensorFlow Training, Horovod, DeepSpeed

### Apache Spark on Kubernetes

**Challenge**: Spark jobs need driver + executors scheduled together. Default scheduler can deadlock when cluster is near capacity.

**Solution**: Gang scheduling with driver and all executors as a pod group.

**Example Workloads**: Spark batch jobs, Spark Structured Streaming, PySpark

### High-Performance Computing (HPC)

**Challenge**: HPC applications are sensitive to memory latency and CPU topology.

**Solution**: NUMA-aware scheduling with isolated or single-NUMA policies for predictable performance.

**Example Workloads**: Molecular dynamics, CFD simulations, finite element analysis

### AI Inference

**Challenge**: Real-time inference needs low latency and consistent performance.

**Solution**: NUMA-aware placement with GPU and CPU on same NUMA node reduces PCIe latency.

**Example Workloads**: Real-time video processing, LLM inference, recommendation systems

### Mixed Workloads

**Challenge**: Clusters run diverse workloadsâ€”microservices, batch jobs, and GPU trainingâ€”each with different needs.

**Solution**: Single scheduler that adapts to workload characteristics without reconfiguration.

**Example**: Production services + nightly batch jobs + ML training on the same cluster

---

## Architecture

KubeNexus is implemented as a set of plugins for the Kubernetes Scheduler Framework:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              KubeNexus Scheduler                        â”‚
â”‚              (Kubernetes Scheduler Framework)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
   â”‚ Gang    â”‚      â”‚   NUMA    â”‚    â”‚  Queue     â”‚
   â”‚Schedulerâ”‚      â”‚ Topology  â”‚    â”‚ Management â”‚
   â”‚         â”‚      â”‚           â”‚    â”‚            â”‚
   â”‚â€¢ Filter â”‚      â”‚ â€¢ Filter  â”‚    â”‚ â€¢ Priority â”‚
   â”‚â€¢ Permit â”‚      â”‚ â€¢ Score   â”‚    â”‚ â€¢ Fairness â”‚
   â”‚â€¢ Reserveâ”‚      â”‚ â€¢ Binding â”‚    â”‚ â€¢ Backfill â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Coscheduling Plugin**: Implements gang scheduling with permit phase coordination
**NUMA Topology Plugin**: Scores nodes based on topology affinity
**Queue Management**: Prevents starvation and ensures fairness

See [Architecture Documentation](docs/architecture.md) for details.

---

## Performance

### Scheduling Latency

| Workload Type | Pods | Latency (p50) | Latency (p99) |
|---------------|------|---------------|---------------|
| Stateless     | 1    | 5ms           | 15ms          |
| Gang (8 pods) | 8    | 50ms          | 150ms         |
| NUMA-aware    | 1    | 8ms           | 25ms          |

### Resource Overhead

| Metric | Value |
|--------|-------|
| Memory | ~50MB |
| CPU    | <0.1 core (idle), <0.5 core (1000 pods/sec) |
| Storage | None (in-memory state only) |

### Scalability

Successfully tested with:
- 1,000 nodes
- 10,000 pods scheduled
- 100 concurrent gang scheduling groups
- Sub-second gang formation time for groups up to 50 pods

---

## Comparison with Alternatives

| Feature | KubeNexus | Volcano | YuniKorn | Kueue |
|---------|-----------|---------|----------|-------|
| **Gang Scheduling** | âœ… Built-in | âœ… Yes | âœ… Yes | âš ï¸ Via adapter |
| **NUMA Topology** | âœ… Full support | âŒ No | âš ï¸ CPU only | âŒ No |
| **GPU Awareness** | âœ… PCIe topology | âš ï¸ Basic | âœ… Yes | âŒ No |
| **Stateless Workloads** | âœ… Native | âš ï¸ Overhead | âš ï¸ Overhead | âŒ Not designed for |
| **Setup Complexity** | ğŸŸ¢ Low | ğŸŸ¡ Medium | ğŸ”´ High | ğŸŸ¡ Medium |
| **External Dependencies** | ğŸŸ¢ None | ğŸŸ¡ CRDs | ğŸ”´ Many | ğŸŸ¡ CRDs |
| **Best For** | Mixed workloads | Batch jobs | Large multi-tenant | Quota management |

### When to Choose KubeNexus

âœ… **Choose KubeNexus if you need:**
- Gang scheduling for distributed ML/Spark/MPI workloads
- NUMA-aware scheduling for GPU/HPC performance
- A single scheduler for stateless + batch + GPU workloads
- Minimal operational complexity
- Quick setup and low resource overhead

âš ï¸ **Consider alternatives if you need:**
- Advanced multi-tenant fair-share policies (â†’ YuniKorn)
- Complex quota hierarchies (â†’ Kueue)
- Workflow orchestration (â†’ Volcano + Argo)
- Support for >5000 node clusters (â†’ YuniKorn)

---

## Documentation

| Document | Description |
|----------|-------------|
| [**User Guide**](docs/USER_GUIDE.md) | Complete guide with examples and troubleshooting |
| [**Kubeflow Integration**](docs/KUBEFLOW_INTEGRATION.md) | Using KubeNexus with Kubeflow Training Operator |
| [**NUMA Scheduling Guide**](docs/NUMA_SCHEDULING_GUIDE.md) | Deep dive into NUMA-aware scheduling |
| [**Quick Reference**](docs/NUMA_QUICK_REFERENCE.md) | Cheat sheet for common tasks |
| [**Scheduler Comparison**](docs/SCHEDULER_COMPARISON.md) | Detailed comparison vs alternatives |
| [**Design Decisions**](docs/DESIGN_DECISIONS.md) | Architecture and API design rationale |
| [**Architecture**](docs/architecture.md) | System design and plugin architecture |

---

## Configuration

### Basic Configuration

KubeNexus works out-of-the-box with sensible defaults. For advanced configuration, edit `config/config.yaml`:

```yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - schedulerName: kubenexus-scheduler
    plugins:
      queueSort:
        enabled:
          - name: Coscheduling
      preFilter:
        enabled:
          - name: Coscheduling
      permit:
        enabled:
          - name: Coscheduling
      score:
        enabled:
          - name: NUMATopology
            weight: 10  # Increase for stronger NUMA preference
```

### Pod Annotations

```yaml
# Gang scheduling (use labels)
labels:
  pod-group.scheduling.kubenexus.io/name: "<group-name>"
  pod-group.scheduling.kubenexus.io/min-available: "<count>"

# NUMA scheduling (use annotations)
annotations:
  numa.scheduling.kubenexus.io/policy: "best-effort|restricted|single-numa|isolated"
  numa.scheduling.kubenexus.io/resources: "cpu,memory,nvidia.com/gpu"
```

---

## Contributing

We welcome contributions! KubeNexus is designed to be simple, focused, and maintainable.

### Development Setup

```bash
# Clone the repository
git clone https://github.com/YOUR_ORG/kubenexus-scheduler.git
cd kubenexus-scheduler

# Install dependencies
go mod download

# Run tests
make test

# Build
make build

# Run locally (requires kubeconfig)
./bin/kubenexus-scheduler --config=config/config.yaml
```

### Areas for Contribution

- **Testing**: Add integration tests for new scenarios
- **Documentation**: Improve guides and examples
- **Features**: Implement requested features (see [Issues](https://github.com/YOUR_ORG/kubenexus-scheduler/issues))
- **Bug fixes**: Fix reported bugs
- **Performance**: Optimize scheduling algorithms

See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## Roadmap

### Current (v1.0)
- âœ… Gang scheduling with permit phase coordination
- âœ… NUMA-aware scheduling with 4 policies
- âœ… Starvation prevention and fairness
- âœ… High availability support

### Planned (v1.5 - Q2 2026)
- â³ Enhanced metrics and monitoring (Prometheus)
- â³ Admission webhook for validation
- â³ Helm chart for easier deployment
- â³ Namespace-based priority configuration

### Future (v2.0 - Q4 2026)
- ğŸ”® Multi-queue support for >5000 node clusters
- ğŸ”® Advanced fair-share policies
- ğŸ”® Dynamic resource reservation
- ğŸ”® Integration with cluster autoscaler

See [GitHub Issues](https://github.com/YOUR_ORG/kubenexus-scheduler/issues) for details and discussions.

---

## Community

- **Issues**: [GitHub Issues](https://github.com/YOUR_ORG/kubenexus-scheduler/issues)
- **Discussions**: [GitHub Discussions](https://github.com/YOUR_ORG/kubenexus-scheduler/discussions)
- **Slack**: [#kubenexus](https://kubernetes.slack.com/messages/kubenexus) on Kubernetes Slack

---

## License

KubeNexus Scheduler is licensed under the [Apache License 2.0](LICENSE).

---

## Acknowledgments

KubeNexus builds upon ideas and patterns from:
- [Kubernetes Scheduler Framework](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
- [Kubernetes Scheduler Plugins](https://github.com/kubernetes-sigs/scheduler-plugins)
- [Volcano Scheduler](https://volcano.sh/)
- [Apache YuniKorn](https://yunikorn.apache.org/)

---

<div align="center">

**[Documentation](docs/) â€¢ [Examples](docs/examples/) â€¢ [Contributing](CONTRIBUTING.md)**

Made with â¤ï¸ for the Kubernetes community

â­ **Star us on GitHub if KubeNexus helps your workloads!**

</div>

